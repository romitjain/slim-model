{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda import empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_hooks():\n",
    "    for handle in activation_handles:\n",
    "        handle.remove()\n",
    "    for handle in attention_handles:\n",
    "        handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "attentions = {}\n",
    "\n",
    "activation_handles = []\n",
    "attention_handles = []\n",
    "\n",
    "def activation_hook(layer_name):\n",
    "    def hook(module, input, output):\n",
    "        if layer_name not in activations:\n",
    "            activations[layer_name] = []\n",
    "        activations[layer_name].append(output.detach().cpu())\n",
    "    return hook\n",
    "\n",
    "def attention_hook(layer_name):\n",
    "    def hook(module, input, output):\n",
    "        _, attn_weights, _ = output\n",
    "        if layer_name not in attentions:\n",
    "            attentions[layer_name] = []\n",
    "        attentions[layer_name].append(attn_weights.detach().cpu())\n",
    "    return hook\n",
    "\n",
    "for i, block in enumerate(model.model.layers):\n",
    "    if hasattr(block, \"mlp\"):\n",
    "        activation_handles.append(block.mlp.register_forward_hook(activation_hook(f\"layer_{i}_mlp\")))\n",
    "    if hasattr(block, \"self_attn\"):\n",
    "        attention_handles.append(block.self_attn.register_forward_hook(attention_hook(f\"layer_{i}_self_attn\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"lighteval/summarization\", \"cnn-dm\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prep_toks(x):\n",
    "    msg = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an helpful AI assistant whose job is to provide a concise summarize of the given content\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": x[\"article\"]\n",
    "        }\n",
    "    ]\n",
    "    return {\n",
    "        \"tokens\": tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "    }\n",
    "\n",
    "dataset = dataset.select(range(128)).map(prep_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = dataset.select_columns(\"tokens\").to_dict()\n",
    "input_prompts = [v for v in tmp.values()][0]\n",
    "all_outputs = []\n",
    "\n",
    "for bs in tqdm(range(0, len(input_prompts), 16)):\n",
    "    be = bs+16\n",
    "    input_tokens = tokenizer(input_prompts[bs:be], return_tensors=\"pt\", truncation=True, padding=True, padding_side=\"left\")\n",
    "    input_tokens = {k: v.cuda() for k, v in input_tokens.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "            max_length=1024\n",
    "        )['sequences'].cpu()\n",
    "        all_outputs.extend(outputs)\n",
    "\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_seq = all_outputs[1]\n",
    "tokenizer.decode(single_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of activation hooks:\n",
    "# [item] -> all steps\n",
    "# item shape: batchsize, seq_len, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Activations:\")\n",
    "for key, value in activations.items():\n",
    "    print(f\"{key}: {len(value)}, {value[0].shape}\")\n",
    "\n",
    "# print(\"\\nAttentions:\")\n",
    "# for key, value in attentions.items():\n",
    "#     print(f\"{key}: {len(value)}, {value[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activation_variance(activation_tensor):\n",
    "    \"\"\"\n",
    "    Compute average variance of activations over tokens.\n",
    "    Expected tensor shape: (batch, seq_len, hidden_dim)\n",
    "    \"\"\"\n",
    "    # Variance along token dimension for each hidden unit\n",
    "    # (if batch >1, variance computed per sample then averaged)\n",
    "    var = activation_tensor.var(dim=1, unbiased=False)  # shape: (batch, hidden_dim)\n",
    "    return var.mean().item()  # average variance across hidden units and batch\n",
    "\n",
    "def compute_attention_entropy(attn_tensor):\n",
    "    \"\"\"\n",
    "    Compute average entropy per head.\n",
    "    Expected tensor shape: (batch, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    eps = 1e-9\n",
    "    # Entropy: -sum(p * log(p)) over the key dimension (last dim)\n",
    "    entropy = - (attn_tensor * torch.log(attn_tensor + eps)).sum(dim=-1)  # (batch, num_heads, seq_len)\n",
    "    # Average over batch and query tokens\n",
    "    avg_entropy = entropy.mean(dim=(0, 2))  # shape: (num_heads,)\n",
    "    return avg_entropy.tolist()\n",
    "\n",
    "def compute_attention_kl(attn_tensor):\n",
    "    \"\"\"\n",
    "    Compute KL divergence of each head's attention distribution with respect to a uniform distribution.\n",
    "    Expected tensor shape: (batch, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    eps = 1e-9\n",
    "    batch, num_heads, seq_len, _ = attn_tensor.shape\n",
    "    uniform = torch.full((seq_len,), 1.0/seq_len, device=attn_tensor.device)\n",
    "    # Compute KL: sum(p * (log(p) - log(u))) for each query token\n",
    "    kl = (attn_tensor * (torch.log(attn_tensor + eps) - torch.log(uniform + eps))).sum(dim=-1)  # (batch, num_heads, seq_len)\n",
    "    avg_kl = kl.mean(dim=(0, 2))  # shape: (num_heads,)\n",
    "    return avg_kl.tolist()\n",
    "\n",
    "import torch\n",
    "import math # Import math\n",
    "\n",
    "def compute_attention_kl_attn_vs_uniform(attn_tensor):\n",
    "    \"\"\"\n",
    "    Compute KL divergence KL(Attention || Uniform) for each head's attention distribution.\n",
    "    Measures how much the attention distribution P deviates from a uniform distribution Q.\n",
    "    KL(P || Q) = sum(P * log(P/Q)) = log(N) - Entropy(P)\n",
    "\n",
    "    attn_tensor: shape (batch, num_heads, seq_len_query, seq_len_key), assumed normalized.\n",
    "    Returns: List of average KL divergence per head. Shape: (num_heads,)\n",
    "    \"\"\"\n",
    "    # Explicitly get all dimensions, especially L_key (last dimension)\n",
    "    B, H, L_query, L_key = attn_tensor.shape\n",
    "    eps = 1e-9 # Small epsilon\n",
    "\n",
    "    # --- FIX: Use L_key for log(N) calculation ---\n",
    "    if L_key <= 0:\n",
    "        # This case should ideally not happen with attention tensors\n",
    "        raise ValueError(f\"Length of attention distribution (L_key) must be positive, but got L_key={L_key}\")\n",
    "    elif L_key == 1:\n",
    "        # If sequence length is 1, KL divergence is 0\n",
    "        log_L = 0.0\n",
    "    else:\n",
    "        # Calculate log(L_key). Use float64 intermediate for precision if desired.\n",
    "        log_L = torch.log(torch.tensor(L_key, device=attn_tensor.device, dtype=torch.float64)).to(attn_tensor.dtype)\n",
    "    # --- End of FIX ---\n",
    "\n",
    "    # Entropy calculation H(P) = -sum(P * log(P + eps))\n",
    "    # Sum over the distribution dimension L_key (the last dimension, -1)\n",
    "    log_term = torch.log(attn_tensor + eps)\n",
    "    entropy = - (attn_tensor * log_term).sum(dim=-1)  # Shape: (B, H, L_query)\n",
    "\n",
    "    # Calculate KL divergence KL(P || Uniform) = log(L_key) - H(P)\n",
    "    # Resulting kl_div tensor has shape corresponding to entropy: (B, H, L_query)\n",
    "    kl_div = log_L - entropy\n",
    "\n",
    "    # Average over batch (dim 0) and query tokens (dim 2, which is L_query)\n",
    "    avg_kl_div = kl_div.mean(dim=(0, 2)) # Shape: (num_heads,)\n",
    "    return avg_kl_div.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_variances = {}\n",
    "attention_entropies = {}\n",
    "attention_kl = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for activations\n",
    "for key, act_list in activations.items():\n",
    "    variances = []\n",
    "    for tensor in act_list:\n",
    "        # Ensure tensor shape is (batch, seq_len, hidden_dim)\n",
    "        variances.append(compute_activation_variance(tensor))\n",
    "    activation_variances[key] = sum(variances) / len(variances)\n",
    "\n",
    "# Compute metrics for attentions\n",
    "for key, attn_list in attentions.items():\n",
    "    entropies = []\n",
    "    kls = []\n",
    "\n",
    "    for tensor in attn_list:\n",
    "        # validate_tensor(tensor)\n",
    "        # Ensure tensor shape is (batch, num_heads, seq_len, seq_len)\n",
    "        entropies.append(compute_attention_entropy(tensor))\n",
    "        kls.append(compute_attention_kl_attn_vs_uniform(tensor))\n",
    "\n",
    "    # Convert list of lists to tensor and average over decoding steps\n",
    "    entropies_tensor = torch.tensor(entropies)  # shape: (num_steps, num_heads)\n",
    "    kl_tensor = torch.tensor(kls)              # shape: (num_steps, num_heads)\n",
    "    avg_entropy = entropies_tensor.mean(dim=0).tolist()\n",
    "    avg_kl = kl_tensor.mean(dim=0).tolist()\n",
    "    attention_entropies[key] = avg_entropy\n",
    "    attention_kl[key] = avg_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the metrics\n",
    "print(\"Activation Variances per layer:\")\n",
    "for key, var in activation_variances.items():\n",
    "    print(f\"{key}: {var}\")\n",
    "\n",
    "print(\"\\nAttention Entropy per head per layer:\")\n",
    "for key, entropy in attention_entropies.items():\n",
    "    print(f\"{key}: {entropy}\")\n",
    "\n",
    "print(\"\\nAttention KL Divergence per head per layer:\")\n",
    "for key, kl in attention_kl.items():\n",
    "    print(f\"{key}: {kl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_composite_scores(activation_variances, attention_entropies, attention_kl, alpha=1.0, beta=1.0):\n",
    "    # Normalize a metric dict to [0, 1] based on its own values.\n",
    "    def normalize(metric_dict):\n",
    "        if not metric_dict:\n",
    "            return {}\n",
    "        values = list(metric_dict.values())\n",
    "        min_val, max_val = min(values), max(values)\n",
    "        if max_val == min_val:\n",
    "            return {k: 0.5 for k in metric_dict}\n",
    "        return {k: (v - min_val) / (max_val - min_val) for k, v in metric_dict.items()}\n",
    "\n",
    "    norm_act = normalize(activation_variances)\n",
    "\n",
    "    norm_att_ent = {}\n",
    "    for l, v in attention_entropies.items():\n",
    "        h_max, h_min = max(v), min(v)\n",
    "        norm_att_ent[l] = {i: (h - h_min) / (h_max - h_min) for i, h in enumerate(v)}\n",
    "\n",
    "    norm_att_kl = {}\n",
    "    for l, v in attention_kl.items():\n",
    "        h_max, h_min = max(v), min(v)\n",
    "        norm_att_kl[l] = {i: (h - h_min) / (h_max - h_min) for i, h in enumerate(v)}\n",
    "\n",
    "    composite_scores = {}\n",
    "    for l, heads in norm_att_ent.items():\n",
    "        composite_scores[l] = {}\n",
    "        for h, v in heads.items():\n",
    "            composite_scores[l][h] = alpha * (1 - v) + beta * (1 - norm_att_kl[l][h])\n",
    "\n",
    "    return norm_act, composite_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_scores, attn_scores = compute_composite_scores(activation_variances, attention_entropies, attention_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_contributions = {}\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "for n, param in model.named_parameters():\n",
    "    weight_contributions[n] = param.numel() / total_params\n",
    "\n",
    "\n",
    "def combine_layer_weights(model_state_dict, layer_num):\n",
    "    layer_prefix = f\"model.layers.{layer_num}.\"\n",
    "    self_attn_prefix = f\"{layer_prefix}self_attn.\"\n",
    "    mlp_prefix = f\"{layer_prefix}mlp.\"\n",
    "    \n",
    "    # Initialize combined weights\n",
    "    combined = {\n",
    "        \"self_attn\": 0.0,\n",
    "        \"mlp\": 0.0,\n",
    "        \"overall\": 0.0\n",
    "    }\n",
    "    \n",
    "    # Count components to calculate average later\n",
    "    counts = {\n",
    "        \"self_attn\": 0,\n",
    "        \"mlp\": 0,\n",
    "        \"overall\": 0.0\n",
    "    }\n",
    "    \n",
    "    # Process all keys in the state dict\n",
    "    for key, value in model_state_dict.items():\n",
    "        if key.startswith(layer_prefix):\n",
    "            combined[\"overall\"] += value\n",
    "            counts[\"overall\"] += 1\n",
    "\n",
    "        if key.startswith(self_attn_prefix):\n",
    "            combined[\"self_attn\"] += value\n",
    "            counts[\"self_attn\"] += 1\n",
    "        elif key.startswith(mlp_prefix):\n",
    "            combined[\"mlp\"] += value\n",
    "            counts[\"mlp\"] += 1\n",
    "\n",
    "    return {k: round(100*v, 2) for k, v in combined.items()}\n",
    "\n",
    "def combine_all_layers(model_state_dict, num_layers=24):\n",
    "    all_layers = {}\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        all_layers[f\"layer_{i}\"] = combine_layer_weights(model_state_dict, i)\n",
    "    \n",
    "    return all_layers\n",
    "\n",
    "combined_weights = combine_all_layers(weight_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attn, title=\"Attention Map\"):\n",
    "    attn = attn.to(torch.float16).cpu().numpy()\n",
    "    plt.imshow(attn, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Key\")\n",
    "    plt.ylabel(\"Query\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, heads in attn_scores.items():\n",
    "    print(l, sum(heads.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 0\n",
    "head_idx = 6\n",
    "step_idx = 0\n",
    "\n",
    "temp = attentions[f'layer_{layer_idx}_self_attn'][step_idx][0, head_idx]\n",
    "print(temp.shape, temp.sum())\n",
    "visualize_attention(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_attention_hook(prune_heads, num_heads):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            attn_output, attn_weights = output[:2]\n",
    "\n",
    "        if prune_heads:\n",
    "            attn_weights[:, prune_heads, :, :] = 0.0\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            return (attn_output, attn_weights) + output[2:]\n",
    "        else:\n",
    "            return attn_weights\n",
    "    return hook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
